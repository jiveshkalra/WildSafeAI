{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "pdb_post_mortem",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Tensorflow_test\\lib\\site-packages\\absl\\app.py:308\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, argv, flags_parser)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 308\u001b[0m   _run_main(main, args)\n\u001b[0;32m    309\u001b[0m \u001b[39mexcept\u001b[39;00m UsageError \u001b[39mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Tensorflow_test\\lib\\site-packages\\absl\\app.py:236\u001b[0m, in \u001b[0;36m_run_main\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls main, optionally with pdb or profiler.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m \u001b[39mif\u001b[39;00m FLAGS\u001b[39m.\u001b[39;49mrun_with_pdb:\n\u001b[0;32m    237\u001b[0m   sys\u001b[39m.\u001b[39mexit(pdb\u001b[39m.\u001b[39mruncall(main, argv))\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Tensorflow_test\\lib\\site-packages\\absl\\flags\\_flagvalues.py:472\u001b[0m, in \u001b[0;36mFlagValues.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m fl:\n\u001b[1;32m--> 472\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(name)\n\u001b[0;32m    473\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m__hiddenflags\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "\u001b[1;31mAttributeError\u001b[0m: run_with_pdb",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 152\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mConverted into Records!!!!: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(output_path))\n\u001b[0;32m    151\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 152\u001b[0m     tf\u001b[39m.\u001b[39;49mapp\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m    154\u001b[0m     \u001b[39m# Extrinsically Yours! \u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\platform\\app.py:36\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Runs the program with an optional 'main' function and 'argv' list.\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m main \u001b[39m=\u001b[39m main \u001b[39mor\u001b[39;00m _sys\u001b[39m.\u001b[39mmodules[\u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmain\n\u001b[1;32m---> 36\u001b[0m _run(main\u001b[39m=\u001b[39;49mmain, argv\u001b[39m=\u001b[39;49margv, flags_parser\u001b[39m=\u001b[39;49m_parse_flags_tolerate_undef)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Tensorflow_test\\lib\\site-packages\\absl\\app.py:321\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, argv, flags_parser)\u001b[0m\n\u001b[0;32m    317\u001b[0m   \u001b[39mraise\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[39m# Check the tty so that we don't hang waiting for input in an\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[39m# non-interactive scenario.\u001b[39;00m\n\u001b[1;32m--> 321\u001b[0m \u001b[39mif\u001b[39;00m FLAGS\u001b[39m.\u001b[39;49mpdb_post_mortem \u001b[39mand\u001b[39;00m sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39misatty():\n\u001b[0;32m    322\u001b[0m   traceback\u001b[39m.\u001b[39mprint_exc()\n\u001b[0;32m    323\u001b[0m   \u001b[39mprint\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Tensorflow_test\\lib\\site-packages\\absl\\flags\\_flagvalues.py:472\u001b[0m, in \u001b[0;36mFlagValues.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    470\u001b[0m fl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flags()\n\u001b[0;32m    471\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m fl:\n\u001b[1;32m--> 472\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(name)\n\u001b[0;32m    473\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m__hiddenflags\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m    474\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: pdb_post_mortem"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#copy test jpg images with xml file and test.csv into same folder naem as test and save this folder into object_detection directory\n",
    "#do same thing for train.csv\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "import argparse\n",
    "import tensorflow.compat.v1 as tf\n",
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "# import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from object_detection.utils import dataset_util\n",
    "from collections import namedtuple, OrderedDict\n",
    "from absl import flags\n",
    "\n",
    "for name in list(flags.FLAGS):\n",
    "  delattr(flags.FLAGS, name)\n",
    "  \n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_string('csv_input', 'C:/Users/user/Documents/GitHub/WildSafeAI/WildEye-4/train/_annotation.csv', 'Path to the CSV input')\n",
    "flags.DEFINE_string('output_path', 'C:/Users/user/Documents/GitHub/WildSafeAI/WildEye-4/', 'Path to output TFRecord')\n",
    "flags.DEFINE_string('image_dir', 'C:/Users/user/Documents/GitHub/WildSafeAI/WildEye-4/train', 'Path to images')\n",
    "FLAGS = flags.FLAGS\n",
    "csv_path = flags.FLAGS.csv_input\n",
    "output_path = flags.FLAGS.output_path\n",
    "\n",
    "# TO-DO replace this with label map\n",
    "def class_text_to_int(row_label):\n",
    "    if row_label == 'bear':\n",
    "        return 1\n",
    "    elif row_label == 'bull':\n",
    "        return 2\n",
    "    elif row_label == 'car':\n",
    "        return 3\n",
    "    elif row_label == 'cat':\n",
    "        return 4\n",
    "    elif row_label == 'cheetah':\n",
    "        return 5\n",
    "    elif row_label == 'cow':\n",
    "        return 6\n",
    "    elif row_label == 'cyote':\n",
    "        return 7\n",
    "    elif row_label == 'deer':\n",
    "        return 8\n",
    "    elif row_label == 'dog':\n",
    "        return 9\n",
    "    elif row_label == 'elephant':\n",
    "        return 10\n",
    "    elif row_label == 'elk':\n",
    "        return 11\n",
    "    elif row_label == 'fox':\n",
    "        return 12\n",
    "    elif row_label == 'giraffe':\n",
    "        return 13\n",
    "    elif row_label == 'goat':\n",
    "        return 14\n",
    "    elif row_label == 'gorilla':\n",
    "        return 15\n",
    "    elif row_label == 'hippopotamus':\n",
    "        return 16\n",
    "    elif row_label == 'human':\n",
    "        return 17\n",
    "    elif row_label == 'jaguar':\n",
    "        return 18\n",
    "    elif row_label == 'lion':\n",
    "        return 19\n",
    "    elif row_label == 'moose':\n",
    "        return 20\n",
    "    elif row_label == 'puma':\n",
    "        return 21\n",
    "    elif row_label == 'raccon':\n",
    "        return 22\n",
    "    elif row_label == 'rhinoceros':\n",
    "        return 23\n",
    "    elif row_label == 'skunk':\n",
    "        return 24\n",
    "    elif row_label == 'squirrel':\n",
    "        return 25\n",
    "    elif row_label == 'tiger':\n",
    "        return 26\n",
    "    elif row_label == 'toy_tiger':\n",
    "        return 27\n",
    "    elif row_label == 'wolf':\n",
    "        return 28\n",
    "    elif row_label == 'zebra':\n",
    "        return 29\n",
    "\n",
    "def split(df, group):\n",
    "    data = namedtuple('data', ['filename', 'object'])\n",
    "    gb = df.groupby(group)\n",
    "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
    "\n",
    "\n",
    "def create_tf_example(group, path):\n",
    "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = Image.open(encoded_jpg_io)\n",
    "    width, height = image.size\n",
    "\n",
    "    filename = group.filename.encode('utf8')\n",
    "    image_format = b'jpg'\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    classes_text = []\n",
    "    classes = []\n",
    "\n",
    "    for index, row in group.object.iterrows():\n",
    "        xmins.append(row['xmin'] / width)\n",
    "        xmaxs.append(row['xmax'] / width)\n",
    "        ymins.append(row['ymin'] / height)\n",
    "        ymaxs.append(row['ymax'] / height)\n",
    "        classes_text.append(row['class'].encode('utf8'))\n",
    "        classes.append(class_text_to_int(row['class']))\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    return tf_example\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    writer = tf.python_io.TFRecordWriter(output_path)\n",
    "    path = os.path.join('C:/Users/user/Documents/GitHub/WildSafeAI/WildEye-4/train')\n",
    "    examples = pd.read_csv(csv_path)\n",
    "    grouped = split(examples, 'filename')\n",
    "    for group in grouped:\n",
    "        tf_example = create_tf_example(group, path)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    writer.close()\n",
    "    output_path = os.path.join(os.getcwd(), output_path)\n",
    "    print('Converted into Records!!!!: {}'.format(output_path))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n",
    "\n",
    "    # Extrinsically Yours! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Downloading Dataset Version Zip in WildEye-4 to tensorflow: 100% [39115280 / 39115280] bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Dataset Version Zip to WildEye-4 in tensorflow:: 100%|██████████| 718/718 [00:02<00:00, 252.95it/s]\n"
     ]
    }
   ],
   "source": [
    "r\"\"\"Training executable for detection models.\n",
    "\n",
    "This executable is used to train DetectionModels. There are two ways of\n",
    "configuring the training job:\n",
    "\n",
    "1) A single pipeline_pb2.TrainEvalPipelineConfig configuration file\n",
    "can be specified by --pipeline_config_path.\n",
    "\n",
    "Example usage:\n",
    "    ./train \\\n",
    "        --logtostderr \\\n",
    "        --train_dir=path/to/train_dir \\\n",
    "        --pipeline_config_path=pipeline_config.pbtxt\n",
    "\n",
    "2) Three configuration files can be provided: a model_pb2.DetectionModel\n",
    "configuration file to define what type of DetectionModel is being trained, an\n",
    "input_reader_pb2.InputReader file to specify what training data will be used and\n",
    "a train_pb2.TrainConfig file to configure training parameters.\n",
    "\n",
    "Example usage:\n",
    "    ./train \\\n",
    "        --logtostderr \\\n",
    "        --train_dir=path/to/train_dir \\\n",
    "        --model_config_path=model_config.pbtxt \\\n",
    "        --train_config_path=train_config.pbtxt \\\n",
    "        --input_config_path=train_input_config.pbtxt\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.6## max GPU occupation\n",
    "K.set_session(tf.Session(config=config))\n",
    "K.get_session().run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "\n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "from object_detection.builders import dataset_builder\n",
    "from object_detection.builders import graph_rewriter_builder\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.legacy import trainer\n",
    "from object_detection.utils import config_util\n",
    "\n",
    "\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_string('master', '', 'Name of the TensorFlow master to use.')\n",
    "flags.DEFINE_integer('task', 0, 'task id')\n",
    "flags.DEFINE_integer('num_clones', 1, 'Number of clones to deploy per worker.')\n",
    "flags.DEFINE_boolean('clone_on_cpu', False,\n",
    "                     'Force clones to be deployed on CPU.  Note that even if '\n",
    "                     'set to False (allowing ops to run on gpu), some ops may '\n",
    "                     'still be run on the CPU if they have no GPU kernel.')\n",
    "flags.DEFINE_integer('worker_replicas', 1, 'Number of worker+trainer '\n",
    "                     'replicas.')\n",
    "flags.DEFINE_integer('ps_tasks', 0,\n",
    "                     'Number of parameter server tasks. If None, does not use '\n",
    "                     'a parameter server.')\n",
    "flags.DEFINE_string('train_dir', '',\n",
    "                    'Directory to save the checkpoints and training summaries.')\n",
    "\n",
    "flags.DEFINE_string('pipeline_config_path', '',\n",
    "                    'Path to a pipeline_pb2.TrainEvalPipelineConfig config '\n",
    "                    'file. If provided, other configs are ignored')\n",
    "\n",
    "flags.DEFINE_string('train_config_path', '',\n",
    "                    'Path to a train_pb2.TrainConfig config file.')\n",
    "flags.DEFINE_string('input_config_path', '',\n",
    "                    'Path to an input_reader_pb2.InputReader config file.')\n",
    "flags.DEFINE_string('model_config_path', '',\n",
    "                    'Path to a model_pb2.DetectionModel config file.')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "@tf.contrib.framework.deprecated(None, 'Use object_detection/model_main.py.')\n",
    "def main(_):\n",
    "  assert FLAGS.train_dir, '`train_dir` is missing.'\n",
    "  if FLAGS.task == 0: tf.gfile.MakeDirs(FLAGS.train_dir)\n",
    "  if FLAGS.pipeline_config_path:\n",
    "    configs = config_util.get_configs_from_pipeline_file(\n",
    "        FLAGS.pipeline_config_path)\n",
    "    if FLAGS.task == 0:\n",
    "      tf.gfile.Copy(FLAGS.pipeline_config_path,\n",
    "                    os.path.join(FLAGS.train_dir, 'pipeline.config'),\n",
    "                    overwrite=True)\n",
    "  else:\n",
    "    configs = config_util.get_configs_from_multiple_files(\n",
    "        model_config_path=FLAGS.model_config_path,\n",
    "        train_config_path=FLAGS.train_config_path,\n",
    "        train_input_config_path=FLAGS.input_config_path)\n",
    "    if FLAGS.task == 0:\n",
    "      for name, config in [('model.config', FLAGS.model_config_path),\n",
    "                           ('train.config', FLAGS.train_config_path),\n",
    "                           ('input.config', FLAGS.input_config_path)]:\n",
    "        tf.gfile.Copy(config, os.path.join(FLAGS.train_dir, name),\n",
    "                      overwrite=True)\n",
    "\n",
    "  model_config = configs['model']\n",
    "  train_config = configs['train_config']\n",
    "  input_config = configs['train_input_config']\n",
    "\n",
    "  model_fn = functools.partial(\n",
    "      model_builder.build,\n",
    "      model_config=model_config,\n",
    "      is_training=True)\n",
    "\n",
    "  def get_next(config):\n",
    "    return dataset_builder.make_initializable_iterator(\n",
    "        dataset_builder.build(config)).get_next()\n",
    "\n",
    "  create_input_dict_fn = functools.partial(get_next, input_config)\n",
    "\n",
    "  env = json.loads(os.environ.get('TF_CONFIG', '{}'))\n",
    "  cluster_data = env.get('cluster', None)\n",
    "  cluster = tf.train.ClusterSpec(cluster_data) if cluster_data else None\n",
    "  task_data = env.get('task', None) or {'type': 'master', 'index': 0}\n",
    "  task_info = type('TaskSpec', (object,), task_data)\n",
    "\n",
    "  # Parameters for a single worker.\n",
    "  ps_tasks = 0\n",
    "  worker_replicas = 1\n",
    "  worker_job_name = 'lonely_worker'\n",
    "  task = 0\n",
    "  is_chief = True\n",
    "  master = ''\n",
    "\n",
    "  if cluster_data and 'worker' in cluster_data:\n",
    "    # Number of total worker replicas include \"worker\"s and the \"master\".\n",
    "    worker_replicas = len(cluster_data['worker']) + 1\n",
    "  if cluster_data and 'ps' in cluster_data:\n",
    "    ps_tasks = len(cluster_data['ps'])\n",
    "\n",
    "  if worker_replicas > 1 and ps_tasks < 1:\n",
    "    raise ValueError('At least 1 ps task is needed for distributed training.')\n",
    "\n",
    "  if worker_replicas >= 1 and ps_tasks > 0:\n",
    "    # Set up distributed training.\n",
    "    server = tf.train.Server(tf.train.ClusterSpec(cluster), protocol='grpc',\n",
    "                             job_name=task_info.type,\n",
    "                             task_index=task_info.index)\n",
    "    if task_info.type == 'ps':\n",
    "      server.join()\n",
    "      return\n",
    "\n",
    "    worker_job_name = '%s/task:%d' % (task_info.type, task_info.index)\n",
    "    task = task_info.index\n",
    "    is_chief = (task_info.type == 'master')\n",
    "    master = server.target\n",
    "\n",
    "  graph_rewriter_fn = None\n",
    "  if 'graph_rewriter_config' in configs:\n",
    "    graph_rewriter_fn = graph_rewriter_builder.build(\n",
    "        configs['graph_rewriter_config'], is_training=True)\n",
    "\n",
    "  trainer.train(\n",
    "      create_input_dict_fn,\n",
    "      model_fn,\n",
    "      train_config,\n",
    "      master,\n",
    "      task,\n",
    "      FLAGS.num_clones,\n",
    "      worker_replicas,\n",
    "      FLAGS.clone_on_cpu,\n",
    "      ps_tasks,\n",
    "      worker_job_name,\n",
    "      is_chief,\n",
    "      FLAGS.train_dir,\n",
    "      graph_hook_fn=graph_rewriter_fn)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
